import requests
import random
import time
import re
import json
import logging
import os
import sys
from logging.handlers import RotatingFileHandler
from datetime import datetime
from sqlalchemy import text
from sqlalchemy.dialects.postgresql import insert

# ğŸ’¡ ç¢ºä¿å¼•å…¥èˆ‡ä½ çš„å°ˆæ¡ˆç›®éŒ„çµæ§‹ä¸€è‡´
from database import SessionLocal
from models import Product, Platform, Price, PriceHistory
from bs4 import BeautifulSoup

# --- 1. æ—¥èªŒé…ç½® (æ¶æ§‹å¸«å¼·åŒ–ç‰ˆ) ---
def setup_logging():
    """
    é…ç½®æ—¥èªŒç³»çµ±ï¼š
    1. å„ªå…ˆä¿è­‰è¼¸å‡ºåˆ°æ§åˆ¶å° (Docker Logs å¿…è¦)
    2. å˜—è©¦å»ºç«‹æª”æ¡ˆæ—¥èªŒï¼Œè‹¥å› æ¬Šé™å•é¡Œå¤±æ•—å‰‡å„ªé›…é™ç´šï¼Œä¸å°è‡´ç¨‹å¼å´©æ½°
    """
    # ä½¿ç”¨çµ•å°è·¯å¾‘é¿å…åœ¨ Docker ç’°å¢ƒä¸­è·¯å¾‘åç§»
    log_dir = os.path.join(os.getcwd(), "logs")
    logger = logging.getLogger("PriceScraper")
    
    # é¿å…é‡è¤‡æ·»åŠ  Handler (åœ¨ Celery æˆ– Uvicorn é‡è¼‰æ™‚å¸¸è¦‹)
    if not logger.handlers:
        logger.setLevel(logging.INFO)
        formatter = logging.Formatter('[%(asctime)s] %(levelname)s (%(name)s): %(message)s', '%Y-%m-%d %H:%M:%S')
        
        # --- A. å»ºç«‹æ§åˆ¶å°è¼¸å‡º (Stdout) ---
        # é€™æ˜¯ Docker çš„ç”Ÿå‘½ç·šï¼Œçµ•å°ä¸æœƒå› ç‚ºæ¬Šé™å•é¡Œå¤±æ•—
        stream_handler = logging.StreamHandler(sys.stdout)
        stream_handler.setFormatter(formatter)
        logger.addHandler(stream_handler)
        
        # --- B. å»ºç«‹æª”æ¡ˆè¼¸å‡º (File) ---
        try:
            if not os.path.exists(log_dir):
                # exist_ok=True é¿å…ä½µç™¼å»ºç«‹æ™‚çš„ Race Condition
                os.makedirs(log_dir, exist_ok=True)
            
            log_path = os.path.join(log_dir, "scraper.log")
            file_handler = RotatingFileHandler(
                log_path, 
                maxBytes=5*1024*1024, 
                backupCount=5, 
                encoding='utf-8'
            )
            file_handler.setFormatter(formatter)
            logger.addHandler(file_handler)
        except (PermissionError, OSError) as e:
            # ğŸ’¡ é—œéµä¿®æ­£ï¼šæ•æ‰æ¬Šé™éŒ¯èª¤ï¼Œå°å‡ºè­¦å‘Šä½†ä¸ä¸­æ­¢ç¨‹å¼
            print(f"âš ï¸  Permission Warning: ç„¡æ³•å¯«å…¥å¯¦é«” Log æª”æ¡ˆ ({e})ã€‚")
            print("ğŸ’¡ æç¤ºï¼šç›®å‰åƒ…æœƒå°‡æ—¥èªŒè¼¸å‡ºè‡³ Docker Console (stdout)ã€‚")
        
        logger.propagate = False
        
    return logger

# å…¨åŸŸåˆå§‹åŒ– Logger
logger = setup_logging()

# --- 2. åƒ¹æ ¼çˆ¬èŸ²å¼•æ“ ---
class PriceScraper:
    def __init__(self):
        self.session = requests.Session()
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Mozilla/5.0 (iPhone; CPU iPhone OS 17_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Mobile/15E148 Safari/604.1",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        ]

    def clean_price(self, price_str):
        if price_str is None: return 0.0
        try:
            clean_str = str(price_str).replace(",", "").replace("$", "").replace("NT", "").strip()
            match = re.search(r'\d+(\.\d+)?', clean_str)
            return float(match.group()) if match else 0.0
        except Exception as e:
            logger.error(f"âš ï¸ åƒ¹æ ¼è½‰æ›å¤±æ•— ({price_str}): {e}")
            return 0.0

    def clean_momo_name(self, raw_name):
        if not raw_name: return ""
        try:
            # è™•ç† Momo ç¶²é å¶ç™¼çš„ç·¨ç¢¼ç•°å¸¸
            return raw_name.encode('utf-8').decode('unicode_escape').encode('latin1').decode('utf-8')
        except:
            return raw_name

    def get_headers(self, platform="Momo"):
        headers = {
            "User-Agent": random.choice(self.user_agents),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
        if platform == "Momo":
            headers["Referer"] = "https://www.momoshop.com.tw/"
        else:
            headers["Referer"] = "https://24h.pchome.com.tw/"
        return headers

    # --- PChome å¼·åŒ–é‚è¼¯ ---
    def scrape_pchome(self, prod_id: str):
        clean_id = str(prod_id).strip()
        logger.info(f"ğŸ” PChome æ·±åº¦çˆ¬å–: {clean_id}")
        
        # 1. å„ªå…ˆä½¿ç”¨ API
        price = self._scrape_pchome_api(clean_id)
        if price: return price

        # 2. API å¤±æ•—å¾Œä½¿ç”¨ç¶²é è§£æä¿åº•
        return self._scrape_pchome_frontend(clean_id)

    def _scrape_pchome_api(self, prod_id):
        try:
            ts = int(time.time() * 1000)
            api_url = f"https://ecapi.pchome.com.tw/ecshop/prodapi/v2/prod?id={prod_id}&fields=Price&_callback=jsonp_price&_={ts}"
            res = self.session.get(api_url, headers=self.get_headers("PChome"), timeout=10)
            match = re.search(r'\((.*)\)', res.text, re.DOTALL)
            if match:
                data = json.loads(match.group(1))
                # å‹•æ…‹å– Key (PChome API å›å‚³çµæ§‹é€šå¸¸ä»¥å•†å“ ID ç‚º Key)
                for key in data.keys():
                    if isinstance(data[key], dict) and "Price" in data[key]:
                        return self.clean_price(data[key]["Price"].get("P", 0))
        except: pass
        return None

    def _scrape_pchome_frontend(self, prod_id):
        """Next.js çµæ§‹è§£æ"""
        url = f"https://24h.pchome.com.tw/prod/{prod_id}"
        try:
            time.sleep(random.uniform(1, 2))
            res = self.session.get(url, headers=self.get_headers("PChome"), timeout=15)
            
            # ç­–ç•¥ï¼šJSON-LD è§£æ (SEO æ¨™æº–çµæ§‹)
            price_match = re.search(r'"price":\s*"(\d+)"', res.text)
            if price_match:
                return float(price_match.group(1))
        except Exception as e:
            logger.error(f"âŒ PChome ç¶²é è§£æå‡ºéŒ¯: {e}")
        return None

    # --- Momo å¼·åŒ–é‚è¼¯ ---
    def scrape_momo(self, i_code: str):
        url = f"https://www.momoshop.com.tw/goods/GoodsDetail.jsp?i_code={i_code}"
        try:
            time.sleep(random.uniform(2, 4))
            res = self.session.get(url, headers=self.get_headers("Momo"), timeout=15)
            if res.status_code != 200: return None

            soup = BeautifulSoup(res.text, 'html.parser')
            # å„ªå…ˆæ‰¾ meta tagï¼Œæœ€å¿«ä¸”ç©©å®š
            meta_price = soup.find("meta", property="product:price:amount")
            if meta_price: 
                return self.clean_price(meta_price.get("content"))
            
            # å‚™æ´ï¼šJSON-LD
            json_ld = soup.find("script", type="application/ld+json")
            if json_ld:
                data = json.loads(json_ld.string)
                offers = data.get('offers')
                if isinstance(offers, list): return self.clean_price(offers[0].get('price'))
                return self.clean_price(offers.get('price'))
        except Exception as e:
            logger.error(f"âŒ Momo æŠ“å–å¤±æ•—: {e}")
        return None

    # --- è³‡æ–™åº«ä¿å­˜é‚è¼¯ ---
    def _save_price_to_db(self, db, item, price_val):
        """
        åŒæ­¥æ›´æ–° Price è¡¨ (Upsert) èˆ‡ PriceHistory è¡¨
        """
        try:
            # 1. æ›´æ–°ç•¶å‰åƒ¹æ ¼
            stmt = insert(Price).values(
                product_id=item.id,
                platform_id=item.platform_id,
                price=price_val,
                updated_at=datetime.now()
            ).on_conflict_do_update(
                index_elements=['product_id'],
                set_={'price': price_val, 'updated_at': datetime.now()}
            )
            db.execute(stmt)

            # 2. å¯«å…¥æ­·å²ç´€éŒ„
            new_history = PriceHistory(
                product_id=item.id,
                platform_id=item.platform_id,
                price=price_val,
                recorded_at=datetime.now()
            )
            db.add(new_history)
        except Exception as e:
            logger.error(f"âŒ DB å¯«å…¥éŒ¯èª¤: {e}")
            raise

    # --- æ ¸å¿ƒå•Ÿå‹•å¼•æ“ ---
    def automated_run(self, target_platform="Momo"):
        logger.info(f"ğŸš€ [TASK] é–‹å§‹æ›´æ–° {target_platform} åƒ¹æ ¼...")
        db = SessionLocal()
        try:
            # ä½¿ç”¨ ILIKE æ¨¡ç³ŠåŒ¹é…å¹³å°åç¨±
            query = text("""
                SELECT p.id, p.name, p.product_id_on_platform, p.platform_id
                FROM products p
                JOIN platforms pl ON p.platform_id = pl.id
                WHERE pl.name ILIKE :target
            """)
            items = db.execute(query, {"target": f"%{target_platform}%"}).fetchall()
            
            if not items:
                logger.warning(f"ğŸ” æ‰¾ä¸åˆ°åŒ¹é… {target_platform} çš„å•†å“ã€‚")
                return

            success_count = 0
            for item in items:
                if "momo" in target_platform.lower():
                    price_val = self.scrape_momo(item.product_id_on_platform)
                else:
                    price_val = self.scrape_pchome(item.product_id_on_platform)

                if price_val and price_val > 0:
                    self._save_price_to_db(db, item, price_val)
                    db.commit() 
                    logger.info(f"âœ… æ›´æ–°: {item.name[:20]}... -> ${price_val}")
                    success_count += 1
                
                # å‹•æ…‹å»¶é²é˜²æ­¢è¢«å° IP
                time.sleep(random.uniform(5, 10))

            logger.info(f"ğŸ ä»»å‹™å®Œæˆ: {success_count}/{len(items)} æˆåŠŸ")

        except Exception as e:
            db.rollback()
            logger.error(f"ğŸ’¥ ä»»å‹™åŸ·è¡Œå´©æ½°: {e}")
        finally:
            db.close()

if __name__ == "__main__":
    scraper = PriceScraper()
    for plat in ["Momo", "PChome"]:
        scraper.automated_run(plat)