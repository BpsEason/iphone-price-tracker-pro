import os
import logging
from celery import Celery
from celery.schedules import crontab  # ğŸ’¡ å¿…é ˆå¼•å…¥ä»¥æ”¯æŒ Cron å®šæ™‚æ ¼å¼
# ğŸ’¡ ç¢ºä¿å¼•å…¥ scraper ä¸­çš„é¡åˆ¥èˆ‡æ—¥èªŒé…ç½®
from scraper import PriceScraper, setup_logging 

# 1. ç¢ºä¿åˆå§‹åŒ–æ—¥èªŒé…ç½®ï¼Œé€™æ¨£ Celery åŸ·è¡Œæ™‚çš„æ—¥èªŒæ‰æœƒåŒæ­¥å¯«å…¥æª”æ¡ˆèˆ‡æ§åˆ¶å°
logger = setup_logging()

# --- 1. Celery åŸºç¤é…ç½® ---
REDIS_URL = os.getenv("REDIS_URL", "redis://redis:6379/0")

celery_app = Celery(
    "tasks",
    broker=REDIS_URL,
    backend=REDIS_URL
)

# ğŸš€ å°ˆæ¥­é…ç½®å„ªåŒ–
celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='Asia/Taipei',
    enable_utc=True,
    # ğŸ’¡ çˆ¬èŸ²é—œéµï¼šPrefetch è¨­ç‚º 1ï¼Œé¿å…å–®å€‹ Worker é ˜å–éå¤šä»»å‹™å°è‡´å…¶ä»– Worker é–’ç½®
    worker_prefetch_multiplier=1,
    task_track_started=True,
    
    # --- ğŸ•’ è‡ªå‹•åŒ–æ’ç¨‹æ ¸å¿ƒé…ç½® (Beat Schedule) ---
    beat_schedule={
        # åç¨±ï¼šå…¨å¹³å°åƒ¹æ ¼å®šæ™‚æ›´æ–°
        'auto-scrape-every-6-hours': {
            'task': 'worker.scrape_all_platforms',  # ğŸ’¡ æŒ‡å‘ä¸‹æ–¹å®šç¾©çš„ Task Name
            'schedule': crontab(minute=0, hour='*/2'), # æ¯ 2 å°æ™‚åŸ·è¡Œä¸€æ¬¡ (0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22 é»)
            # æ¸¬è©¦ç”¨ (æ¯ 5 åˆ†é˜è·‘ä¸€æ¬¡)ï¼š'schedule': 300.0, 
        },
    },
    
    # é™åˆ¶é »ç‡ï¼Œä¿è­· IP ä¸è¢«é›»å•†å°é–
    task_annotations={
        'worker.scrape_all_platforms': {'rate_limit': '1/m'} 
    }
)

# --- 2. å®šç¾© Celery Tasks ---

# ğŸ’¡ é¡¯å¼æŒ‡å®š name="worker.scrape_all_platforms" ä»¥ç¢ºä¿ Scheduler æ´¾ç™¼èˆ‡ Worker æ¥æ”¶ä¸€è‡´
@celery_app.task(
    bind=True, 
    name="worker.scrape_all_platforms", 
    max_retries=3, 
    default_retry_delay=300
)
def scrape_all_platforms(self):
    """
    æ’ç¨‹ä»»å‹™ï¼šåŸ·è¡Œå…¨å¹³å°åƒ¹æ ¼æ›´æ–°
    """
    logger.info("ğŸ“… [Celery] æ¥æ”¶åˆ°æ’ç¨‹ä»»å‹™ï¼šé–‹å§‹å…¨å¹³å°çˆ¬å–")
    
    try:
        # åœ¨ä»»å‹™å…§éƒ¨å¯¦ä¾‹åŒ–ï¼Œç¢ºä¿è³‡æ–™åº«é€£ç·šç¨ç«‹
        scraper = PriceScraper()
        for platform in ["Momo", "PChome"]:
            logger.info(f"æ­£åœ¨è™•ç†å¹³å°: {platform}")
            scraper.automated_run(platform)
            
        return {"status": "success", "msg": "All platforms updated"}
    except Exception as exc:
        logger.error(f"âŒ å…¨å¹³å°ä»»å‹™åŸ·è¡Œå¤±æ•—: {exc}")
        raise self.retry(exc=exc)

@celery_app.task(
    bind=True, 
    name="worker.scrape_single_product_task", 
    max_retries=2
)
def scrape_single_product_task(self, platform_name, product_id_on_platform):
    """
    å–®ä¸€å•†å“å³æ™‚çˆ¬å–ä»»å‹™ (é€šå¸¸ç”± API æ‰‹å‹•è§¸ç™¼)
    """
    logger.info(f"âš¡ [Celery] å³æ™‚æ›´æ–°æŒ‡ä»¤ï¼š{platform_name} (ID: {product_id_on_platform})")
    try:
        scraper = PriceScraper()
        if platform_name.lower() == "momo":
            price = scraper.scrape_momo(product_id_on_platform)
        else:
            price = scraper.scrape_pchome(product_id_on_platform)
            
        if price and price > 0:
            logger.info(f"âœ… å³æ™‚æŠ“å–æˆåŠŸï¼šåƒ¹æ ¼ ${price}")
            return {"status": "success", "price": price}
        else:
            logger.warning(f"âš ï¸ æŠ“å–çµæŸï¼Œä½†æœªç²å¾—æœ‰æ•ˆåƒ¹æ ¼ (ID: {product_id_on_platform})")
            return {"status": "failed", "reason": "Price not found"}
    except Exception as exc:
        logger.error(f"âŒ å³æ™‚ä»»å‹™ç•°å¸¸: {exc}")
        raise self.retry(exc=exc)